{"cells":[{"cell_type":"code","source":["#Step 1: Import and Read Data + Convert to DF\nclean_mc = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/clean_mc/export__1_-d35a2.csv')\n\nclean_mc.printSchema()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["clean_mc.createOrReplaceTempView(\"clean_mc\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%sql DROP TABLE IF EXISTS trans"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sql\nCREATE TABLE trans\nFROM clean_mc"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["dataset = spark.table(\"trans\")\ncols = dataset.columns\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["spark.range(1).createOrReplaceTempView(\"dataset\")\ntype(spark.table(\"dataset\"))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import trim\nfrom pyspark.sql.functions import split\ndf = dataset.withColumn(\"purch_location\", trim(dataset.purch_location))\ndf.show(5)\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Remove ON from purch_location and null values and any other weird values split purch_location in order to separate unnecessary string, will delete in future\nfrom pyspark.sql.functions import split\nsplitcol = split(df['purch_location'], ' ')\ndf1 = df.withColumn('purch_loc', splitcol.getItem(0))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#Generate a list of common cities traveled to in Canada, if column1 does not equal any string from that list then state it as GTA\n#Above failed, replaced blank spots with GTA (Will work better for prediction purposes to have several values in one column)\nfrom pyspark.sql.functions import col, when\n\ndef blank_as_null(x):\n    return when(col(x) != \"\", col(x)).otherwise('GTA')\n\ndf2 = df1.withColumn(\"purch_loc\", blank_as_null(\"purch_loc\"))\n\n#Dropping uneccesary columns\ndrop_list = ['purch_location', 'column2', 'id', 'purch_date']\ndf3=df2.select([column for column in df2.columns if column not in drop_list])\ndf3.show(5)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml import Pipeline\n# categorical columns\ncategorical_columns = [\"purch_class\", \"serv-provider\", \"purch_loc\"]"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Attempt 2 String Indexing, One Hot Encoding and Vector Assembling\nstages = []\nfor catCol in categorical_columns:\n  stringIndexer = StringIndexer(inputCol=catCol, outputCol=catCol+\"Index\")\n  \n  encoder = OneHotEncoder(inputCol=catCol+\"Index\", outputCol=catCol+\"classVec\")\n  \n  stages += [stringIndexer, encoder]\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["label_stringIdx = StringIndexer(inputCol = \"y\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["numCols = [\"tot_amt\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categorical_columns) + numCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["pipeline = Pipeline(stages=stages)\ncols=df3.columns\n\npipeline_model = pipeline.fit(df3)\nfinal_columns = cols + ['features', 'label']\ndf_p = pipeline_model.transform(df3).\\\n            select(final_columns)\n\ndisplay(df_p)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["training, test = df_p.randomSplit([0.8, 0.2], seed=123)\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=4)\n\n# Train model with Training Data\ndtModel = dt.fit(training)\nprint \"numNodes = \", dtModel.numNodes\nprint \"depth = \", dtModel.depth"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["predictions = dtModel.transform(test)\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"tot_amt\", \"purch_class\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n\ndt.getImpurity()\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1,2,3,4])\n             .addGrid(dt.maxBins, [20,40,80])\n             .build())"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(training)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["print \"numNodes = \", cvModel.bestModel.numNodes\nprint \"depth = \", cvModel.bestModel.depth"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["predictions = cvModel.transform(test)\nevaluator.evaluate(predictions)\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"tot_amt\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Decision Trees on cc Transactions - FINAL","notebookId":1939235898047138},"nbformat":4,"nbformat_minor":0}
